{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3b163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports loaded successfully!\n",
      "ğŸ“ Ready to process tagged region sermon content\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Core libraries\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import Document, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import BaseNode, TextNode\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… All imports loaded successfully!\")\n",
    "print(\"ğŸ“ Ready to process tagged region sermon content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdd1f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loaded 1 documents from './devo_dir'\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"./devo_dir\", recursive=True)\n",
    "documents = reader.load_data()\n",
    "print(f\"ğŸ“„ Loaded {len(documents)} documents from './devo_dir'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3284770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Parsed 4 nodes from documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "EMBEDDING_DIMENSIONS = 3072\n",
    "\n",
    "\n",
    "parser = SimpleNodeParser(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=20,\n",
    "        include_metadata=True,\n",
    "        include_prev_next_rel=True\n",
    "    )\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(f\"ğŸ“ Parsed {len(nodes)} nodes from documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1964435",
   "metadata": {},
   "outputs": [],
   "source": "# Process and embed all document chunks\nprint(\"ğŸ”„ Processing document chunks for embedding...\")\n\ndef process_and_embed_documents(documents, nodes, openai_client, pinecone_index):\n    \"\"\"\n    Process document nodes, generate embeddings, and upsert to Pinecone with metadata.\n    \"\"\"\n    vectors_to_upsert = []\n    \n    for i, node in enumerate(nodes):\n        print(f\"ğŸ“ Processing chunk {i+1}/{len(nodes)}...\")\n        \n        # Generate embedding for the node text\n        response = openai_client.embeddings.create(\n            input=node.text,\n            model=EMBEDDING_MODEL\n        )\n        embedding = response.data[0].embedding\n        \n        # Create metadata for the chunk\n        metadata = {\n            'text': node.text,\n            'source_document': node.metadata.get('file_name', 'unknown'),\n            'chunk_index': i,\n            'char_count': len(node.text),\n            'document_id': node.metadata.get('doc_id', str(node.id_)),\n            'file_path': node.metadata.get('file_path', ''),\n            'creation_date': node.metadata.get('creation_date', ''),\n        }\n        \n        # Prepare vector for upsert\n        vector_data = {\n            'id': f\"chunk_{i}_{node.id_}\",\n            'values': embedding,\n            'metadata': metadata\n        }\n        \n        vectors_to_upsert.append(vector_data)\n    \n    # Batch upsert to Pinecone\n    print(f\"ğŸ“¤ Upserting {len(vectors_to_upsert)} vectors to Pinecone...\")\n    response = pinecone_index.upsert(vectors=vectors_to_upsert)\n    \n    print(f\"âœ… Successfully processed and upserted {len(vectors_to_upsert)} document chunks\")\n    return response\n\n# Initialize clients\nclient = OpenAI()\npc = Pinecone()\naog_index = pc.Index(\"aog-devo\")\n\n# Process all documents\nupsert_response = process_and_embed_documents(documents, nodes, client, aog_index)\nprint(f\"ğŸ“Š Upserted vectors count: {upsert_response.upserted_count}\")"
  },
  {
   "cell_type": "code",
   "id": "bj2slbgezqa",
   "source": "def test_rag_functionality(pinecone_index, openai_client, test_query: str = \"What does the Bible teach about faith?\"):\n    \"\"\"\n    Test the RAG functionality for retrieving relevant devotional content.\n    \"\"\"\n    print(f\"ğŸ” Testing RAG functionality with query: '{test_query}'\")\n    \n    try:\n        # Create embedding for the test query\n        query_response = openai_client.embeddings.create(\n            input=test_query,\n            model=EMBEDDING_MODEL\n        )\n        query_embedding = query_response.data[0].embedding\n        \n        # Search Pinecone for similar content chunks\n        search_response = pinecone_index.query(\n            vector=query_embedding,\n            top_k=3,\n            include_metadata=True,\n            include_values=False\n        )\n        \n        print(f\"ğŸ¯ Found {len(search_response.matches)} relevant chunks\")\n        \n        # Display results\n        for i, match in enumerate(search_response.matches, 1):\n            print(f\"\\nğŸ“„ Result {i} (Similarity Score: {match.score:.4f}):\")\n            if match.metadata:\n                source = match.metadata.get('source_document', 'Unknown source')\n                chunk_idx = match.metadata.get('chunk_index', 'N/A')\n                char_count = match.metadata.get('char_count', 'N/A')\n                \n                print(f\"   ğŸ“ Source: {source} | Chunk: {chunk_idx} | Length: {char_count} chars\")\n                print(f\"   ğŸ“– Content: {match.metadata.get('text', 'No text available')[:300]}...\")\n                print(\"   \" + \"-\" * 80)\n            else:\n                print(\"   âŒ No metadata available\")\n        \n        return search_response\n        \n    except Exception as e:\n        print(f\"âŒ Error testing RAG functionality: {str(e)}\")\n        return None\n\n# Test the complete RAG functionality with devotional content\nprint(\"ğŸ§ª Testing complete RAG pipeline...\")\ntest_queries = [\n    \"What does the Bible teach about faith?\",\n    \"How can Scripture guide my daily life?\",\n    \"What are the foundations of faith?\"\n]\n\nfor query in test_queries:\n    print(\"\\n\" + \"=\"*80)\n    test_result = test_rag_functionality(aog_index, client, query)\n    \nprint(\"\\nâœ… Complete RAG functionality testing finished\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}